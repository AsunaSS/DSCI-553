## Competition Winners
1. Zhang Zhiyu (0.9421705226027092)
2. Li Chaoyu(me), Wang Yunhe
3. Tamazian Alain, Minoofar Amir
4. Tian Jiayu
5. Ankit Nitin Kumar Shah
6. Lin Chih-Hsien

## My Pipeline
The method I finally submitted has three steps: preprocessing, training and fine-tuning, and testing. 

My final RMSE on Test set is around **0.9679**(Thanks my lucky star:heart:)

### Preprocessing:
In the preprocessing stage, after many experiments, I finally only selected the data in **user.json** and **business.json**. I first embed the entire dataset in a graph. Here I choose *city, flatted category, user, and business* as the points in the graph (of course you can try *attributes* but I don't think it will work better). I used the **[HIN2Vec](https://dl.acm.org/doi/10.1145/3132847.3132953)** model for graph embedding because this graph is a heterogeneous graph with millions of points. For heterogeneous graphs, HIN2Vec has better adaptability than traditional algorithms such as LINE or Node2Vec (I will also show the metrics of these two model below). I merged the embedding features generated by HIN2Vec with other common features of users and businesses. For categorical features, I use LabelEncoder for encoding, and I normalize numerical features.
***
***Time consuming***: It needs 38-42 hours to finish all the preprocess work on one 3090Ti
***

### Training and Fine-tuning:
In the training phase, I used **Xgbregressor** to train all the integrated features and use the **Optuna** library to further fine-tune the model to get the best hyperparameters. After getting the optimal hyperparameters, I used 10-fold to cross-validate the training set, and the final predicted value will take the average of the cross-validation. I trained a total of four different models to deal with the cold start problem of user and business on the embedding dataset, user dataset, and business dataset. 
- There are totally seven different situations:
1. The user and the business are both in the embedding graph - Model1
2. The user is in the embedding graph but the business is not in the embedding graph - Model2
3. The user is not in the embedding graph and the business is in the embedding graph - Model3
4. Nor the user or the business is in the embedding graph, but the user is in **user.json**, the business is in **business.json** - Model4
5. The user is in **user.json**, but business is not in **business.json** - Use user's average stars
6. The user is not in **user.json**, and business is in **business.json** - Use business's average stars
7. Nor the user or the business is in the json file - Use average star of the overall dataset
***
***Time consuming***: It needs 23-25 hours to finish all the training and fine-tuning work on one 3090Ti
***

### Testing:
In the test phase, I combined the train and validation data sets for training so that the model can further learn the data characteristics on the validation set. This is ok in this project because we will eventually test on the test dataset. This is also my last submitted version.
***
***Time consuming***: 700~900s
***

:zap:I also tried some different methods, I showed their performance here::zap:
### Directly Encoding:
You can draw a heatmap of the correlation between different features and ratings, then choose the top 10 to 12 features to train and fine-tune your Xgbregressor. The best performance is **0.9758** on Val set. If you train it on both train set and val set, the peroformance will be **0.9138** but you cannot trust it.
### LINE Graph Embedding:
A traditional graph embedding method. The paper is [here](https://arxiv.org/abs/1503.03578). You can just replace the HIN2Vec model in my pipeline with LINE model. The best performance is **0.9734** on Val set.
### Node2Vec Graph Embedding:
Another traditional graph embedding method. The paper is [here](https://arxiv.org/abs/1607.00653). You can also just replace the HIN2Vec model in my pipeline with LINE model. The best performance is **0.9725** on Val set.
